---
# title: 'When does l1 minimization succeed for sparse signal recovery?'
# date: 2022-04-11 17:39:00
# description: This post is my reading summarization from Prof. Yi Ma's new book [High-dimensional data analysis with low-dimensional models](https://book-wright-ma.github.io).
# permalink: /posts/2022/04/blog-post-1/
# tags: formatting tables
# categories: sample-posts
# giscus_comments: true
# related_posts: true
# datatable: true
layout: post
title: When does l1 minimization succeed for sparse signal recovery?
date: 2022-04-11 17:39:00
description: This post is my reading summarization from Prof. Yi Ma's new book [High-dimensional data analysis with low-dimensional models](https://book-wright-ma.github.io).
tags: formatting tables
categories: sample-posts
giscus_comments: true
related_posts: true
datatable: true
---

This post is my reading summarization from Prof. Yi Ma's new book [High-dimensional data analysis with low-dimensional models](https://book-wright-ma.github.io).

Suppose the observed signal $y$ is generated from an unknown sparse signal $x_0$ by a linear mapping.
The goal of the sparse signal recovery problem is to recover the sparse signal $x_0$ from the observed signal $y$.
Specifically, it requires solving the following optimization problem:

$$
P_0
\begin{cases}
&\min ||x||_0 \\ 
& \text{s.t.} Ax=y 
\end{cases}
$$

where $x\in\mathbb{R}^m$ is the recovered signal, $y\in\mathbb{R}^n$ is the observed signal, and $A=[a_1|\cdots|a_n]\in\mathbb{R}^{m\times n}$ is the linear mapping.
<!-- Many real-world applications require to solve the sparse signal recovery problem.
For example, we want to recover the true image ($x$) from a corrupted one ($y$). -->
However, $P_0$ is NP-hard due to its combinational nature. 
In practice, we often relax the $L_0$ minimization to $L_1$ minimization as:

$$
P_1
\begin{cases}
&\min ||x||_1 \\ 
& \text{s.t.} Ax=y 
\end{cases}
$$

**This post tries to informally introduce 
(1)  when solving $P_0$ is meaningful in the sense that its solution is exactly the true sparse signal $x_0$;
(2) when solving $P_1$ gives the exact solution to $P_0$.**
# When the solution of $P_0$ is exactly $x_0$?

Suppose the true sparse signal is $x_0$ with $\|\|x_0\|\|_0\leq k$, and the observed siginal $y$ is generated by $y=Ax_0=\sum_i a_i x_0(i)$.
Let $\hat{x}$ be the solution to $P_0$. In this section we analyze when $\hat{x} == x_0$.

Here we begin with the motivating analysis. 
The observed siginal $y$ equals $Ax_0=\sum_i a_i x_0(i)$.
Different columns of $A$ are mixed and we only have the access to the mixed results.
Intuitively, it is easier to guess which column participates in this linear combination if the columns of $A$ are orthogonal to each other.

Below we connect the above intuition to the sparse signal recovery problem.
Since $\hat{x}$ is the solution of $P_0$, we have $||\hat{x}||_0 \leq ||x_0||_0\leq k$.
Let $\delta = \hat{x} - x_0$ be the error signal.
We note that $\delta$ always lies in the null space of $A$ because $A\delta=A\hat{x}-Ax_0=0$.
We also notice that

$$
||\delta||_0 = ||\hat{x} - x_0||_0 \leq ||\hat{x}||_0 + ||x_0||_0\leq 2k
$$

To recover any sparse signal $x_0$ with $\|\|x_0\|\|_0\leq k$, it means that we have $\delta=\bf{0}$ for all $\|\|x_0\|\|_0\leq k$.
Mathmatically, it means *the only $\delta\in$ null($A$) with $\|\|\delta\|\|_0 \leq 2k$ is $\delta=\bf{0}$*. Or equalvently, every set of $2k$ columns of $A$ is linearly independent/orthognal.

Here we introduce the concept of Kruskal Rank: *The Kruskal Rank of a matrix A, written as Krank(A), is the largest number r such that every set of r columns of A is orthogonal*.
Based on the concept, below we formally summarize the above results:


**($l_0$ minimization) Suppose that $y=Ax_0$ with $\|\|x_0\|\|\leq \frac{1}{2}$ Krank($A$).
Then $x_o$ is the unique optimal solution to the $l_0$ minimization problem $P_0$.**

## Mutual coherence and the Kruskal Rank

As we introduced above, the Kruskal Rank determines when the solution of $P_0$ is accurate.
However, as we mentioned at the beginning, $P_0$ is NP-hard.
Before we discussing when the solution of $P_1$ is exactly that of $P_0$, we first introduce the concept of *coherence* here, which is useful in the next section.
From the high-level, both the Kruskal Rank and coherence charactizes the "orthogonality" of A, but from different perspective. The coherence $\mu(A)$ is defined as

$$
\mu(A) = \max_{i\neq j}|\langle \frac{a_i}{||a_i||_2}, \frac{a_j}{||a_j||_2} \rangle|.
$$

According to [Wikipedia](https://en.wikipedia.org/wiki/Mutual_coherence_(linear_algebra)), the coherence is the maximum absolute value of the cross-correlations between the columns of $A$.
To get a sense, we notice that $0 \leq \mu(A) \leq 1$ and  $\mu(A)=0$ when $A$ is an orthogornal matrix.
 
Later in next section, we will see that the coherence is very important to answer the question: **When solving $P_1$ succeed for sparse signal recovery?**

Below we first connect the coherence with the Kruskal Rank.
Let $\mathcal{S}\subset [n]$ be a set of indices with $|\mathcal{S}|=k$, and $A_{\mathcal{S}}\in\mathbb{R}^{m\times k}$ is the sub-matrix selected according to $\mathcal{S}$.
Since the matrix rank is invariant to the normalization, 
we normalize $A_\mathcal{S}$ such that $A_{\mathcal{S}}=[\frac{a_1}{\|\|a_1\|\|_2}|\cdots\|\frac{a_k}{\|\|a_k\|\|_2}]$.

Suppose $A_{\mathcal{S}}^\top A_{\mathcal{S}}=I+\Delta$. 
Here $\Delta$ contains only the off-dianoal entries as $\langle \frac{a_i}{\|\|a_i\|\|_2}, \frac{a_j}{\|\|a_j\|\|_2}\rangle$ at position $(i, j)$. We have:

$$
||\Delta||_2 \leq ||\Delta||_F \leq k||\Delta||_\infty \leq k\mu(A)
$$


Since the matrix $L_2$ norm follows the triangle inequality, we have:

$$
\delta_{max}(A_{\mathcal{S}}^\top A_{\mathcal{S}})\leq 1 + k\mu(A).
$$

Similarly, we have:

$$
1 - k\mu(A)\leq \delta_{min}(A_{\mathcal{S}}^\top A_{\mathcal{S}})\leq \delta_{max}(A_{\mathcal{S}}^\top A_{\mathcal{S}})\leq 1 + k\mu(A)
$$

Intuitively, if $\mu(A)$ is small, then $A$ is well conditioned in the sense that its smallest sigular value $\delta_{min}$ is not far from the largest sigular value $\delta_{max}$.

From above, if $k\mu(A)\leq 1$, then $A_{\mathcal{S}}$ has full column rank for all $\mathcal{S}$ with $|\mathcal{S}|=k$.
In other words, Krank($A$) $\geq k$.
Since $0\leq \mu(A) \leq 1$, we have Krank($A$)$\geq k \geq \frac{1}{\mu(A)}$.

We have shown that if the sparse signal $x_0$ satisfies $\|\|x_0\|\|\leq \frac{1}{2}$ Krank($A$), then solving $P_0$ will recover the original sparse signal.
In the next section, we will show that if $x_0$ is "sparser" in the sense that we tigen the bound from $\|\|x_0\|\|_0\leq \frac{1}{2}$ Krank($A$) to $\|\|x_0\|\|_0\leq \frac{1}{2\mu(A)}$, then solving $P_1$ can also accurately recover $x_0$.


# When solving $P_1$ succeed for sparse signal recovery?

As we mentioned in the last section, when $\|\|x_0\|\|_0\leq \frac{1}{2\mu(A)}$, solving $P_1$ will recover $x_0$.
Here we give the proof sketch.

Following the notation in the book, we use $\partial\|\|\cdot\|\|_1(x_0)$ to denote the subgradient of the $L_1$ norm.
By the subgradient inequality, for any $v\in \partial\|\|\cdot\|\|_1(x_0)$ and $\hat{x}\in\mathbb{R}^n$, we have

$$
\|\hat{x}\|_1 \geq \|x_0\|_1 + \langle v, \hat{x} - x_0 \rangle
$$

If $\hat{x}$ is the solution to $P_1$, then $A(\hat{x}-x_0)=0$. Hence we have 
$$
\langle A^\top \lambda, \hat{x}-x_0\rangle = \langle \lambda, A^\top (\hat{x}-x_0)\rangle = 0
$$

So if we produce a $\lambda\in\mathbb{R}^m$ such that $A^\top \lambda\in\partial\|\|\cdot\|\|_1(x_0)$, then we have $\|\|\hat{x}\|\|_1 \geq \|\|x_0\|\|_1$, which means $x_0$ is the unique optimal solution to $P_1$.
Below we introduce how to find such a $\lambda$.

We use the subscript $\bf{|}$ to denote the support of $x_0$.
Namely, for all $i\in\bf{|}$, $x_0(i)\neq 0$.
Similarly, for all $i\in\bf{|^c}$, $x_0(i)=0$.
Here we note that the subgradient of $L_1$ norm is:

$$
\partial\|\cdot\|_1(x_0) = \{v|v_{\bf{|}}=sign(x_0), \|v\|_\infty\leq 1\}
$$

Hence, the condition $A^\top\lambda\in\partial\|\|\cdot\|\|_1(x_0)$ becomes:

$$
\begin{cases}
A_{\bf{|}}^\top\lambda=sign(x_0)_{\bf{|}} \\
\|A_{\bf{|}^c}^\top\lambda\|_\infty\leq 1
\end{cases}
$$

Here we simply construct the $\lambda$ as 

$$
\lambda=A_{\bf{|}}(A_{\bf{|}}^\top A_{\bf{|}})^{-1}sign(x_0)_{\bf{|}}
$$

It is easy to show that such a $\lambda$ automatically satisfies the first condition.
Below we show that the constructed $\lambda$ satisfies $\|\|A_{\bf{|}^c}^\top\lambda\|\|_\infty\leq 1$ when $\|\|x_0\|\|_0\leq \frac{1}{2\mu(A)}$.
Specifically, we need to calculate

$$
\|A_{\bf{|}^c}^\top\lambda\|_\infty = \|A_{\bf{|}^c}A_{\bf{|}}(A_{\bf{|}}^\top A_{\bf{|}})^{-1}sign(x_0)_{\bf{|}}\|_\infty
$$

For each element $j$, we have:
$$
\begin{align}
|a_j^\top A_{\bf{|}}(A_{\bf{|}}^\top A_{\bf{|}})^{-1}sign(x_0)_{\bf{|}}| 
&\leq\|A_{|}^\top a_j\|_2 \|(A_{\bf{|}}^\top A_{\bf{|}})^{-1}\|_2 \|sign(x_0)_{\bf{|}}|\|_2 \\
&\leq \sqrt k\mu(A) \frac{1}{1-k\mu(A)}\sqrt k\\
&= \frac{k\mu(A)}{1-k\mu(A)} \\
&\leq 1
\end{align}
$$

The above inequality follows from the fact that $\|\|x_0\|\|_0=k\leq \frac{1}{2\mu(A)}$

