---
layout: about
title: About
permalink: /
subtitle:

profile:
  align: right
  image: profile.jpeg
  image_circular: false # crops the image to make it circular
  address: >
    <p>Duncan Hall, 3014</p>
    <p>6100 Main St</p>
    <p>Houston, TX 77005</p>

news: false  # includes a list of news items
latest_posts: false  # includes a list of the newest posts
selected_papers: false # includes a list of papers marked as "selected={true}"
social: true  # includes social icons at the bottom of the page
---

### About me

Iâ€™m Zirui "Ray" Liu, a Ph.D. student from the Department of [Computer Science at Rice University](https://cs.rice.edu/), co-advised by Dr. [Xia "Ben" Hu](https://cs.rice.edu/~xh37/index.html) and Prof. [Vladimir Braverman](https://cs.rice.edu/~vb21/).

My core research interests lie in **large-scale machine learning and machine learning system**. I co-design algorithm and system, aiming to scale-up and/or accerlate machine learning models. Some examples include [LLM KV Cache quantization](https://arxiv.org/pdf/2402.02750), [randomized matrix mulplication for LLMs](https://arxiv.org/abs/2305.15265), and [GNN quantization](https://openreview.net/pdf?id=vkaMaq95_rX). Besides MLsys, I am also interested in other core problems/applications in LLMs like RAG and LLM safety.

<!-- Recently, I've been exploring ways to train, [fine-tune](https://arxiv.org/abs/2305.15265), and [deploy](https://browse.arxiv.org/pdf/2305.11186.pdf) **Large Language Models (LLMs)** on commodity hardware, especially making them more accessible to academics. -->

**I will join the Department of Computer Science at University of Minnesota Twin Cities as a tenure-track Assistant Professor.**

Feel free to reach out if you would like to collaborate on MLSys or on-device ML research.

ðŸ“§ðŸ“§ Prospective students: **I am looking for self-motivated students with strong background in coding and ML**. Feel free to drop me a line to [ziruiliu.recruit@gmail.com](ziruiliu.recruit@gmail.com) together with resume and transcripts if you are interested.

### News
<!-- - Our new work [KIVI](https://arxiv.org/pdf/2402.02750.pdf) shows that 2-bit is enough for representing KV Cache! It can improve inference throughput by 3X and provide direct support for long context tasks. Please check our [code](https://github.com/jy-yuan/KIVI?tab=readme-ov-file) to have a try. -->

- ðŸ”¥ðŸ”¥ Our [KIVI](https://arxiv.org/pdf/2402.02750.pdf) largely inspires the [KV Cache quantization system in Huggingface](https://huggingface.co/docs/transformers/main/en/generation_strategies#kv-cache-quantization). Code is available [here](https://github.com/jy-yuan/KIVI); And our [Self-Extend](https://arxiv.org/abs/2401.01325) is highlighted during [Google I/O session](https://www.youtube.com/watch?v=TV7qCk1dBWA). Code is available [here](https://colab.research.google.com/drive/1jtaOyPOUQh7QksTVIV8oOvDKRGf0NrAZ).


- Our [KIVI](https://arxiv.org/pdf/2402.02750.pdf), [Self-Extend](https://arxiv.org/abs/2401.01325), and [Compress-then-prompt](https://arxiv.org/abs/2305.11186) are accepted by ICML 2024.


- Our [Memory-Efficient LLM fine-tuning](https://arxiv.org/abs/2305.15265) work is covered by [Rice CS New](https://csweb.rice.edu/news/rice-cs-xia-ben-hu-investigates-llms-and-likely-applications).

- Three paper accepted to Neurips 2023.

- Two paper accepted to TMLR.

- Twos papers accepted to ICML 2023.

- One paper accepted to MLSys 2023.

- Two papers accepted to Neurips 2022, DreamShard and GNN Benchmark (Benchmark track).


### Publications

Please refer to [publications](https://scholar.google.com/citations?user=0i1w_egAAAAJ) or [Google Scholar](https://scholar.google.com/citations?user=0i1w_egAAAAJ).