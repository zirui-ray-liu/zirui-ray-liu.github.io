---
layout: about
title: About
permalink: /
subtitle:

profile:
  align: right
  image: profile.jpeg
  image_circular: false # crops the image to make it circular

news: false  # includes a list of news items
latest_posts: false  # includes a list of the newest posts
selected_papers: false # includes a list of papers marked as "selected={true}"
social: true  # includes social icons at the bottom of the page
---

### About me

Iâ€™m Zirui "Ray" Liu, an assistant professor from the Department of [Computer Science at UMN](https://cse.umn.edu/cs). Previously I graduated from [Computer Science at Rice University](https://cs.rice.edu/), where I worked with Prof. [Xia Hu](https://cs.rice.edu/~xh37/index.html) and Prof. [Vladimir Braverman](https://www.cs.jhu.edu/~vova/).

I am mostly interested in *Large Language Models* and their applications, focusing on enabling them to **combine and process information from diverse sources and domains**.  For that reason I deeply care about **efficiency, reasoning, long-context ability, and understanding their inner working mechanism**. I also enjoy **extending foundation models to other domains**, exploring the interplay between different source of data.



<!-- - **Core LLM:** like improving long-context/reasoning/retrieval/memory ability; designing experiments to understand LLMs. -->

<!-- -  **Efficent LLM:** Making LLMs more accessible through hardware-aware approaches, like optmizing compression, implementation, training strategies.  -->

<!-- I strongly believe that a good system design requires a deep understanding of the workload, along with a thorough knowledge of the capabilities and limitations of current LLMs. -->

<!-- - **Extending Transformers beyond Text:** exploring the interplay between Transformers and various domains, like Graphs, Proteins, Gene, and Healthcare. -->

<!-- My core research interests lie in **large-scale machine learning and machine learning system**. I co-design algorithm and system, aiming to scale-up and/or accerlate machine learning models. Some examples include [LLM KV Cache quantization](https://arxiv.org/pdf/2402.02750), [randomized matrix mulplication](https://arxiv.org/abs/2305.15265), and [GNN quantization](https://openreview.net/pdf?id=vkaMaq95_rX). Besides MLsys, I am also interested in other **core problems/applications in LLMs like RAG, long context ability, and LLM safety.** -->


<!-- **<span style="color: red">I will join the CS Department at University of Minnesota Twin Cities as an Assistant Professor in Fall 2024</span>** -->

<!-- Feel free to reach out if you would like to collaborate on LLM, MLSys or on-device ML research. -->


ðŸ“§ðŸ“§ **<span style="color: red">Recruiting</span>: I am always looking for PhD students and research interns with strong coding skills**. Feel free to drop me a line to [ziruiliu dot recruit at gmail dot com](mailto:ziruiliu.recruit@gmail.com) together with resume, transcripts, and a short description of why you'd like to work with me.

### News

- Four paper accepted at EMNLP 2025 (1 **Oral**, 3 Findings). Three paper accepted at Neurips 2025 (1 **Oral**, 2 Poster). Kudo to my students and collaborators.


- 2025/7. We are organizing the [VISION workshop](https://vision-workshop.github.io/iccv-2025/) at ICCV about industrial inspection.

- Gave a talk ([Paper](https://arxiv.org/abs/2506.09501), [Record](https://www.youtube.com/watch?v=xtzACc7qbyI&feature=youtu.be), [Slide](https://asap-seminar.github.io/assets/slides/Numerical_Precision.pdf)) at [ASAP seminar](https://asap-seminar.github.io/) about the impact of numerical precision to LLM reasoning evaluation.

- Received NSF CIRC planning, NSF NAIRR Pilot, UMN DSI Internal Funding, and Adobe Gifts. Thanks NSF, UMN DSI, and Adobe!

- One paper accepted at ICML 2025. Previously In [KIVI](https://arxiv.org/pdf/2402.02750.pdf) we observed K cache has outlier channels, while V doesn't. In this [ICML 2025 paper](https://arxiv.org/abs/2502.01563), we found that this observation is caused by RoPE.

- Giving one tutorial at AAAI 2025 about KV Cache Optimization. Slide can be found [here](https://github.com/henryzhongsc/longctx_bench/issues/6)

- One paper accepted at CVPR 2025 about Structured Pruning

- Two paper accepted at ICLR 2025. One about LLM-based file system and one about zero-order fine-tuning of LLMs

- [KVCache Compression Benchmark](https://arxiv.org/pdf/2407.01527) accepted at EMNLP24. If you want to know the research landspace of this area, take a look at the [paper](https://arxiv.org/pdf/2407.01527) and [code](https://github.com/henryzhongsc/longctx_bench).

- Introduced a [rare disease question-answering (ReDis-QA) dataset](https://huggingface.co/datasets/guan-wang/ReDis-QA) to assess the chatbot ability of diagnosing rare diseases.

- ðŸ”¥ðŸ”¥ Our [KIVI](https://arxiv.org/pdf/2402.02750.pdf) largely inspires the [KV Cache quantization system in Huggingface](https://huggingface.co/docs/transformers/main/en/internal/generation_utils#transformers.QuantizedCache). Code is available [here](https://github.com/jy-yuan/KIVI); And our [Self-Extend](https://arxiv.org/abs/2401.01325) is used in [Llama.cpp](https://github.com/ggerganov/llama.cpp/pull/4815), implemented by [KerasNLP](https://github.com/google-gemini/gemma-cookbook/blob/main/Gemma/Self_extend_Gemma.ipynb), and highlighted during [Google I/O session](https://www.youtube.com/watch?v=TV7qCk1dBWA&t=2025s). Code is available [here](https://github.com/datamllab/LongLM).


- Our [KIVI](https://arxiv.org/pdf/2402.02750.pdf), [Self-Extend](https://arxiv.org/abs/2401.01325), and [Compress-then-prompt](https://openreview.net/forum?id=muBJPCIqZT) are accepted by ICML 2024. [Self-Extend](https://arxiv.org/abs/2401.01325) has been selected as <span style="color: red"><span style="color: red">Spotlight (3.5%)</span></span> at ICML2024!


<!-- - Our [Memory-Efficient LLM fine-tuning](https://arxiv.org/abs/2305.15265) work is covered by [Rice CS New](https://csweb.rice.edu/news/rice-cs-xia-ben-hu-investigates-llms-and-likely-applications). -->

<!-- - Three paper accepted to Neurips 2023.

- Two paper accepted to TMLR.

- Twos papers accepted to ICML 2023.

- One paper accepted to MLSys 2023.

- Two papers accepted to Neurips 2022, DreamShard and GNN Benchmark (Benchmark track). -->


### Publications

Please refer to [publications](https://scholar.google.com/citations?user=0i1w_egAAAAJ) or [Google Scholar](https://scholar.google.com/citations?user=0i1w_egAAAAJ).