<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://zirui-ray-liu.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://zirui-ray-liu.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-11-10T20:54:05+00:00</updated><id>https://zirui-ray-liu.github.io/feed.xml</id><title type="html">Zirui’s Homepage</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Implementing Flash-Attention with Softmax Offset (Sinks)</title><link href="https://zirui-ray-liu.github.io/blog/2025/FA+Offset/" rel="alternate" type="text/html" title="Implementing Flash-Attention with Softmax Offset (Sinks)"/><published>2025-08-23T00:00:00+00:00</published><updated>2025-08-23T00:00:00+00:00</updated><id>https://zirui-ray-liu.github.io/blog/2025/FA+Offset</id><content type="html" xml:base="https://zirui-ray-liu.github.io/blog/2025/FA+Offset/"><![CDATA[<p><a href="https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf">GPT-OSS</a> have released and one notable feature is that each attention head now includes a learned bias term in the denominator of the softmax. This is similar to techniques like <a href="https://www.evanmiller.org/attention-is-off-by-one.html">off-by-one attention</a> and <a href="https://arxiv.org/abs/2309.17453">attention sinks</a>.</p> <p>Mathmatically, this is equvelent to</p> \[P_i = \frac{e^{S_i}}{\sum_j e^{S_j} + \textcolor{red}{\text{Bias}}}\] <p>This <code class="language-plaintext highlighter-rouge">Bias</code> is a learnable positive variable assigned to each attention head. Intuitively, it provides the model with the option to ‘attend to nothing. To facilitate fast training and inference with this Softmax mechanism, we need to integrate it into <a href="https://github.com/Dao-AILab/flash-attention">Flash-Attention</a> OP.</p> <p>For the ease of understanding and implementation, in this blog I will modify the <a href="https://triton-lang.org/main/getting-started/tutorials/06-fused-attention.html">official triton FA2 tutorial</a> to add this function. My own implementation is available here: <a href="https://github.com/zirui-ray-liu/FA2-with-Attn-Offset-Triton">https://github.com/zirui-ray-liu/FA2-with-Attn-Offset-Triton</a></p> <h3 id="flash-attention-2-forward-pass">Flash-Attention-2 Forward Pass:</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fa2-fwd-480.webp 480w,/assets/img/fa2-fwd-800.webp 800w,/assets/img/fa2-fwd-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/fa2-fwd.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Diagram of how FlashAttention2 forward pass is performed. The outer loop is Q&amp;O, inner loop is K&amp;V. Source: https://arxiv.org/pdf/2307.08691. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fa2-fwd-algo-480.webp 480w,/assets/img/fa2-fwd-algo-800.webp 800w,/assets/img/fa2-fwd-algo-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/fa2-fwd-algo.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The modified FA2 forward pass algorithm with learnable headwise bias. </div> <h3 id="flash-attention-2-backward-pass">Flash-Attention-2 Backward Pass:</h3> <p>In the forward pass, the attention softmax is implemented with base-2 exponentials (In the triton implementation, it uses base-2 exponentials instead of base-e since base-2 is much more hardware friendly via bit-shifting): \(l_i = \sum_j 2^{s_{ij} - m_i}\) and at the epilogue: \(l_i \leftarrow l_i + \text{bias} \cdot 2^{-m_i}\)</p> <p>Stored log-sum-exp (LSE): \(L_i = m_i + \log_2(l_i) = \log_2\!\big(\sum_j e^{s_{ij}} + \text{bias}\big)\)</p> <p>Output: \(o_i = \frac{\sum_j 2^{s_{ij} - m_i} v_j}{l_i}\)</p> <p>Define: \(\Delta_i = \langle o_i, do_i \rangle = \sum_k o_{ik}\, do_{ik}\)</p> <p>In the original FA2 backward, $\Delta_i$ will be explicitly calculated before obtaining \(dQ, dK, dV\).</p> <p>Since the bias appears only in the denominator, the gradient w.r.t. bias is: \(\frac{\partial \mathcal{L}}{\partial \text{bias}} = -\sum_i \frac{\Delta_i}{\sum_j e^{s_{ij}} + \text{bias}} = -\sum_i \Delta_i 2^{-M_i}.\)</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fa2-bwd-algo-480.webp 480w,/assets/img/fa2-bwd-algo-800.webp 800w,/assets/img/fa2-bwd-algo-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/fa2-bwd-algo.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The modified FA2 backward pass algorithm with learnable headwise bias. </div> <p>Note this is implementation is not the most hardware-efficient one. But it is easy to understand and can be implemented with minimal line of changes.</p>]]></content><author><name></name></author><category term="Infrastructure"/><category term="Infrastructure"/><summary type="html"><![CDATA[GPT-OSS have released and one notable feature is that each attention head now includes a learned bias term in the denominator of the softmax. This is similar to techniques like off-by-one attention and attention sinks.]]></summary></entry><entry><title type="html">LLMs are secretly lossless text compressor and how to use it like one</title><link href="https://zirui-ray-liu.github.io/blog/2024/llmzip/" rel="alternate" type="text/html" title="LLMs are secretly lossless text compressor and how to use it like one"/><published>2024-11-22T00:00:00+00:00</published><updated>2024-11-22T00:00:00+00:00</updated><id>https://zirui-ray-liu.github.io/blog/2024/llmzip</id><content type="html" xml:base="https://zirui-ray-liu.github.io/blog/2024/llmzip/"><![CDATA[<p>Many people say that machine learning and data compression are two sides of the same coin. While this comparison is often debated, in this blog, I will connect Large Language Models (LLMs) with XZ (yes, that compression software), and how to enpower these compression software with LLMs to achieve much higher compression ratio losslessly, from the information theory perspective.</p> <p>This blog post is largerly inspired by <a href="https://blog.wtf.sg/posts/2023-06-05-yes-its-just-doing-compression.-no-its-not-the-diss-you-think-it-is./">Shawn Tan’s blog</a> and <a href="https://bellard.org/nncp/">NNCP project</a></p> <p>My own implementation is available here: <a href="https://github.com/zirui-ray-liu/text-compressor">https://github.com/zirui-ray-liu/text-compressor</a></p> <h2 id="huffman-encoding-with-minimal-bitlength">Huffman Encoding with minimal bit length</h2> <p>When you put English text into a computer, it saves every individual character as one btye of data. In the early days of computing, disk space for storing data was very limited. This led to a fundamental problem: <em>What is the minimal bit length for storing a text file losslessly?</em></p> <p>In information theory, finding the minimal description length means encoding data as efficiently as possible. In the above example, “Shannon” contains 5 unique characters (S, h, a, n, o), we can use 3 bits per character to distinguish each of them. With 7 characters in total, this encoding would require 7×3=21 bits. During decoding, we just greedy match the bit to characters from left to right. Can we do better?</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/huffman_encode-480.webp 480w,/assets/img/huffman_encode-800.webp 800w,/assets/img/huffman_encode-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/huffman_encode.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Encoding each char with 3 bits. Source: https://www.youtube.com/watch?v=B3y0RsVCyrw&amp;t=544s </div> <p>Since the total bit length equals</p> \[\sum_w \text{Freq}(w) \cdot \text{BitLength}(w)\] <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/huffman_encode2-480.webp 480w,/assets/img/huffman_encode2-800.webp 800w,/assets/img/huffman_encode2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/huffman_encode2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Variational length encoding. Source: https://www.youtube.com/watch?v=B3y0RsVCyrw&amp;t=544s </div> <p>One intuitive way to reduce the total bit length is to <em>assign fewer bits to common characters and more bits to rarer ones</em>, a concept known as variable-length encoding. For example, if “n” is the most frequent character in “Shannon,” we might assign it just 1 bit, while assigning other characters longer bit sequences. In this optimized encoding, we reduce the total bit length to 14 bits.  In practice, Huffman coding is an algorithm to help you find this optimal bit assignment automatically. It first count the frequency of all characters as input and assign the bit according to this frequency table. This technique forms the foundation of most modern compression tools, like XZ and Gzip, which use Huffman coding to reduce file sizes efficiently.</p> <h2 id="why-it-is-linked-tollms">Why it is linked to LLMs?</h2> <p>Huffman coding assigns bit according to the frequency table of each possible character. In other words, it assigns bits according to the empirical distribution observed from the text file. Some implicit assumption behinds it are:</p> <ul> <li>The character distribution is a multinomial distribution with</li> </ul> \[P(w) = \frac{\# w}{\# \text{Total Chars}}\] <ul> <li>It treats different character locations equally with this multinomial distribution</li> </ul> <p>We know that these assumptions are not hold in practice since language has structures. So how about we replace these character distribution with one given by a LLM? Let us analyze what will happen in this case:</p> <p>LLMs are trained with the next word prediction, in other words, they are optimized for</p> \[\min_{\theta} \log P_{\theta}(w_T|w_1,\cdots, w_{T-1})\] <p>where \(w_T\) are the next word to be predicted and \(w_1, \cdots, w_{T-1}\) are the previous workds. Let’s assume that the LLM is well-trained and can assign a high probability to the correct next word (much higher than a simple frequency-based probability derived from counting characters). In this scenario, Huffman coding becomes relevant. Recall that Huffman coding works by assigning fewer bits to more probable symbols, which means that more common characters (or words, in this case) get shorter codes. Therefore, if the LLM can assign a higher probability to the correct next word (compared to a basic character frequency count), it will effectively reduce the number of bits needed to represent that word, thanks to the principles of Huffman coding. <strong>From this perspective, LLMs are exactly trained to minimize description length for text files losslessly!</strong></p> <h2 id="implementation">Implementation</h2> <p>Below is the psudo code of using LLMs for compressing text files.</p> <pre><code class="language-pseudocode">\begin{algorithm}
\caption{Compression}
\begin{algorithmic}
\STATE Given a text sequence $w_1,\cdots,w_{T-1}$
\STATE Predict the $w_T$ distribution $P(w_T|w_1,\cdots, w_{T-1})$ with LLM 
\STATE Convert this distribution into pseudo frequency table by \texttt{Round}(10,000 * $P(w_T|w_1,\cdots, w_{T-1})$)
\STATE Construct the Huffman Tree with the frequency table and encode $w_T$ with the leaf bits
\STATE Repeat this process until all words are encoded
\end{algorithmic}
\end{algorithm}
</code></pre> <pre><code class="language-pseudocode">\begin{algorithm}
\caption{Decompression}
\begin{algorithmic}
\STATE Given a bitstream and the text sequence $w_1,\cdots,w_{T-1}$ we have decompressed so far
\STATE Predict the $w_T$ distribution $P(w_T|w_1,\cdots, w_{T-1})$ with LLM 
\STATE Convert this distribution into pseudo frequency table by \texttt{Round}(10,000 * $P(w_T|w_1,\cdots, w_{T-1})$)
\STATE Construct the Huffman Tree with the frequency table
\STATE Walk the Huffman tree and match the next available bits in the bitstream.
\STATE Once we hit a leaf, decode the word $w_T$ and add it to the text sequence
\STATE Repeat this process until all bits in the bitstream are matched
\end{algorithmic}
\end{algorithm}
</code></pre> <p>I implement the above algorithms, which is modified from the source code of <a href="https://bellard.org/nncp/">NNCP project</a>. The code is available here: <a href="https://github.com/zirui-ray-liu/text-compressor">https://github.com/zirui-ray-liu/text-compressor</a></p> <p>This project is implemented in pure Python, leveraging the Huggingface Transformer API. We provide an example showcasing how GPT-2 can be used to compress a small text corpus from Wikipedia.</p> <p>To compress the text, run the following command:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bash scripts/eval.sh
</code></pre></div></div> <p>For the example text file <code class="language-plaintext highlighter-rouge">unittest.txt</code> (size: 4KB),the script should generate a compressed file <code class="language-plaintext highlighter-rouge">unittest.bin</code> (size: 542 Bytes) and a decompressed file <code class="language-plaintext highlighter-rouge">decmp_unittest.txt</code>.</p> <p>For reference, using XZ with the command <code class="language-plaintext highlighter-rouge">xz -9 unittest.txt</code> produces a file <code class="language-plaintext highlighter-rouge">unittest.txt.xz</code> (size: 1.9KB), which is 3.5X larger than GPT2 based version.</p> <h3 id="notes">Notes</h3> <ul> <li> <p>You can greatly improve the compression ratio with stronger LLMs</p> </li> <li> <p>You can use this framework to losslessly compress anything that can be formulated as next-something prediction</p> </li> </ul>]]></content><author><name></name></author><category term="LLM"/><category term="LLM"/><summary type="html"><![CDATA[Many people say that machine learning and data compression are two sides of the same coin. While this comparison is often debated, in this blog, I will connect Large Language Models (LLMs) with XZ (yes, that compression software), and how to enpower these compression software with LLMs to achieve much higher compression ratio losslessly, from the information theory perspective.]]></summary></entry><entry><title type="html">Rounding Errors of FP16 and its impact in Machine Learning</title><link href="https://zirui-ray-liu.github.io/blog/2024/rounding-error/" rel="alternate" type="text/html" title="Rounding Errors of FP16 and its impact in Machine Learning"/><published>2024-11-22T00:00:00+00:00</published><updated>2024-11-22T00:00:00+00:00</updated><id>https://zirui-ray-liu.github.io/blog/2024/rounding-error</id><content type="html" xml:base="https://zirui-ray-liu.github.io/blog/2024/rounding-error/"><![CDATA[<p><a href="https://en.wikipedia.org/wiki/Half-precision_floating-point_format">FP16</a> and <a href="https://en.wikipedia.org/wiki/Bfloat16_floating-point_format">BF16</a> are two most commonly used numerical precision in Machine Learning. In this blog we will discuss some abnormal behaviors of them.</p> <h3 id="quick-example-with-torch-260">Quick example (with torch 2.6.0):</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span><span class="kn">import</span> <span class="n">torch</span>
<span class="o">&gt;&gt;&gt;</span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="mf">8.125</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float16</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="mi">1032</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float16</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span><span class="n">a</span> <span class="o">+</span> <span class="mi">1024</span> <span class="o">==</span> <span class="n">b</span>
<span class="nf">tensor</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <h3 id="why-this-happens">Why this happens</h3> <p>To understand this abnormal behavior, you first need to understand how computer encodes floating point numbers.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/IEEE_754r_Half_Floating_Point_Format.svg-480.webp 480w,/assets/img/IEEE_754r_Half_Floating_Point_Format.svg-800.webp 800w,/assets/img/IEEE_754r_Half_Floating_Point_Format.svg-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/IEEE_754r_Half_Floating_Point_Format.svg.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>So in FP16, it has 1 sign bit, 5 exponent bits, and 10 mantissa bits. Every number can be expressed as</p> \[x = (-1)^S * 2^{(E-15)} * (1+\frac{M}{2^{10}}),\] <p>where S, E, and M are the decimal of the sign, exponent, and mantissa, respectively. For example,</p> <p>The binary code of 8.125 in FP16 is 0100100000010000. In this case, S = 0, E = 18 (10010), M = 16 (0000010000), so</p> \[x = (-1)^0 * 2^{(18-15)} * (1+\frac{16}{2^{10}}) = 8.125\] <p>Now let’s examine what happens to 8.125 + 1024.</p> <p>let <strong>b = 8.125 + 1024</strong>.</p> <p>We know that the sign part S of b is 0. The exponent part E of b must be 25, since 25 - 15 == 10 (otherwise you cannot represent a number &gt; 1024 &amp;&amp; &lt; 2048).</p> <p>Now let’s examine the expression of b.</p> \[b = 2^{(25-15)} * (1+\frac{M_b}{2^{10}}) = 2^{(25-15)} + 2^{(25-15-10+{M_b})} = 1024 + 2^{M_b}\] <p>So if we write b in FP16, b is an integer! The original \(.125\) of \(8.125\) is rounded off in this case, which explain the beginning example.</p> <p>For BF16, it assigns 8bits to exponent and 7bits to mantissa. So it does not the above failure patterns, however,<strong>it sacrifices the precision of fraction part</strong>.</p> <h3 id="impacts-to-model-training">Impacts to model training</h3> <p>This can get really bad when you perform model training since we update the gradient every step. In the worst case, this can leads to uncovergence (imagine every step you round off a small portion of weight parameters).</p> <p>Thus, it is crusial to use <strong>a higher precision accumulator</strong>, which is the common practice in mixed precision training, i.e., we maintain a FP32 copy of model weights and optimizer states.</p>]]></content><author><name></name></author><category term="Infrastructure"/><category term="Infrastructure"/><summary type="html"><![CDATA[FP16 and BF16 are two most commonly used numerical precision in Machine Learning. In this blog we will discuss some abnormal behaviors of them.]]></summary></entry></feed>